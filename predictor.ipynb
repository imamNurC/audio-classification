{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy.stats\n",
    "from sklearn import metrics, preprocessing\n",
    "import pandas as pd\n",
    "from keras.utils import audio_dataset_from_directory\n",
    "from scipy.io import wavfile\n",
    "from IPython import display\n",
    "import joblib\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# # Define the path to the dataset directory\n",
    "# DATASET_PATH = \"C:/Users/ITPKL/Desktop/pydev/ccw_sound\"\n",
    "# data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "# # Create training and validation dataset\n",
    "# train_ds, val_ds =  audio_dataset_from_directory(\n",
    "#     directory=data_dir,\n",
    "#     batch_size=64,\n",
    "#     validation_split=0.3,\n",
    "#     seed=0,\n",
    "#     output_sequence_length=16000,\n",
    "#     subset='both'\n",
    "# )\n",
    "\n",
    "# # Retrieve the class names from the training dataset\n",
    "# label_names = np.array(train_ds.class_names)\n",
    "# label_names_val = np.array(val_ds.class_names)\n",
    "\n",
    "\n",
    "# # Print the label names\n",
    "# print()\n",
    "# print(\"label names training:\", label_names)\n",
    "# print(\"label names testing:\", label_names_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove singleton dimensions from audio tensor\n",
    "# def squeeze(audio, labels):\n",
    "#     audio = tf.squeeze(audio, axis=-1)\n",
    "#     return audio, labels\n",
    "\n",
    "# # Apply the squeeze function to the training and validation datasets\n",
    "# train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "# val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "# print(train_ds)\n",
    "\n",
    "\n",
    "# # Define the fixed length for FFTs\n",
    "# fft_length = 1024\n",
    "\n",
    "# # Initialize lists to store magnitude of FFTs, corresponding labels, and mask\n",
    "# fft_magnitude_data = []\n",
    "# labels = []\n",
    "# masks = []\n",
    "\n",
    "# # fft_magnitude_val = []\n",
    "# # labels_val = []\n",
    "# # masks_val = []\n",
    "\n",
    "# Define a function to read WAV files and convert to FFT\n",
    "def read_wav_file(file_path, fft_length=1024):\n",
    "    try:\n",
    "        sample_rate, audio = wavfile.read(file_path)\n",
    "        if len(audio) == 0:\n",
    "            return None\n",
    "        return np.fft.fft(audio)[:fft_length]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading WAV file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process a single WAV file\n",
    "def proses_file_audio(file_path, fft_length=1024):\n",
    "    fft_magnitude_data = []\n",
    "    masks = []\n",
    "    fft = read_wav_file(file_path, fft_length)\n",
    "    if fft is not None:\n",
    "        fft_magnitude = np.abs(fft)\n",
    "        mask = np.ones_like(fft_magnitude)\n",
    "        mask[len(fft_magnitude):] = 0\n",
    "        fft_magnitude_padded = np.pad(fft_magnitude, (0, fft_length - len(fft_magnitude)))\n",
    "        mask_padded = np.pad(mask, (0, fft_length - len(mask)))\n",
    "        \n",
    "        # Append the padded FFT magnitude and mask to their respective lists\n",
    "        fft_magnitude_data.append(fft_magnitude_padded)\n",
    "        masks.append(mask_padded)\n",
    "\n",
    "    # Initialize an empty list to store the padded masks\n",
    "    padded_masks = [np.pad(mask, (0, fft_length - len(mask))) for mask in masks]\n",
    "\n",
    "    # Find the maximum length among all padded masks\n",
    "    max_length = max(len(mask) for mask in padded_masks)\n",
    "\n",
    "    # Pad each mask to the maximum length\n",
    "    padded_masks = [np.pad(mask, (0, max_length - len(mask))) for mask in padded_masks]\n",
    "\n",
    "    # Convert the lists to numpy arrays\n",
    "    fft_magnitude_data = np.array(fft_magnitude_data)\n",
    "    padded_masks = np.array(padded_masks)\n",
    "\n",
    "    # Print the shapes of the resulting arrays\n",
    "    print(\"FFT Magnitude Data Shape:\", fft_magnitude_data.shape)\n",
    "    print(\"Masks Shape:\", padded_masks.shape)\n",
    "\n",
    "    # Create a DataFrame with the magnitude of the FFT outputs and numerical labels\n",
    "    fft_magnitude_df = pd.DataFrame(fft_magnitude_data)\n",
    "    mask_df = pd.DataFrame(padded_masks)\n",
    "    # Rename the columns of the DataFrame to fft1, fft2, ...\n",
    "    # new_columns = [f'fft{n}' for n in range(1, fft_length + 1)]\n",
    "    # fft_magnitude_df.columns = new_columns\n",
    "\n",
    "    # Add a new column 'Label' to the DataFrame based on the folder name\n",
    "    # fft_magnitude_df['label'] = labels\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # display.display(fft_magnitude_df)\n",
    "\n",
    "    fft_magnitude_tuples = [tuple(row) for row in fft_magnitude_df.values]\n",
    "    random.shuffle(fft_magnitude_tuples)\n",
    "\n",
    "    # Display the shuffled list of tuples\n",
    "    # for tup in fft_magnitude_tuples:\n",
    "    # print(tup)\n",
    "\n",
    "    \n",
    "\n",
    "    #sudah terbentuk numpy array  \n",
    "    print(fft_magnitude_data)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(padded_masks)\n",
    "    print()\n",
    "\n",
    "    # belum terbentuk numpy array dan masih dalam bentuk dataframe\n",
    "    print(fft_magnitude_df)\n",
    "    print()\n",
    "    print(mask_df)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc_X = StandardScaler()\n",
    "    scaled_input = sc_X.fit_transform(fft_magnitude_df)\n",
    "\n",
    "    # Load the model\n",
    "    model = joblib.load('C:/Users/ITPKL/Desktop/pydev/model2024-01-30.pkl')\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(scaled_input)\n",
    "    print(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFT Magnitude Data Shape: (1, 1024)\n",
      "Masks Shape: (1, 1024)\n",
      "[[5556.         3700.48378365 1121.95707109 ...  100.37376141\n",
      "    79.10443937   14.2990863 ]]\n",
      "\n",
      "\n",
      "[[1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      "     0            1            2            3            4           5     \\\n",
      "0  5556.0  3700.483784  1121.957071  3197.812507  1060.270372  126.851977   \n",
      "\n",
      "        6           7           8           9     ...       1014       1015  \\\n",
      "0  527.65781  201.075725  178.882286  904.589214  ...  19.582697  62.648178   \n",
      "\n",
      "        1016       1017       1018       1019       1020        1021  \\\n",
      "0  40.478346  18.521002  83.638313  41.777428  34.032539  100.373761   \n",
      "\n",
      "        1022       1023  \n",
      "0  79.104439  14.299086  \n",
      "\n",
      "[1 rows x 1024 columns]\n",
      "\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  1014  \\\n",
      "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   1.0   \n",
      "\n",
      "   1015  1016  1017  1018  1019  1020  1021  1022  1023  \n",
      "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
      "\n",
      "[1 rows x 1024 columns]\n",
      "\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# manggil input\n",
    "proses_file_audio('ccw/ng/CCW_297.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def dframe ():\n",
    "#     # Create a DataFrame with the magnitude of the FFT outputs and numerical labels\n",
    "#     fft_magnitude_df = pd.DataFrame(fft_magnitude_data)\n",
    "\n",
    "#     # Rename the columns of the DataFrame to fft1, fft2, ...\n",
    "#     # new_columns = [f'fft{n}' for n in range(1, fft_length + 1)]\n",
    "#     # fft_magnitude_df.columns = new_columns\n",
    "\n",
    "#     # Add a new column 'Label' to the DataFrame based on the folder name\n",
    "#     # fft_magnitude_df['label'] = labels\n",
    "\n",
    "#     # Display the DataFrame\n",
    "#     # display.display(fft_magnitude_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes all fft values as X and drops the Label\n",
    "# X = fft_magnitude_df.drop(['label'], axis=1)\n",
    "# # y = fft_magnitude_df['label']\n",
    "\n",
    "# input_1_image = np.array(X)\n",
    "# label_1_image = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splits the datasset into 80% training set and 20% test set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# Splits the dataset into 80% training set and 20% test set\n",
    "# input_latih, input_coba, label_latih, label_coba = train_test_split(input_1_image, label_1_image, test_size=0.2, random_state=0)\n",
    "\n",
    "# Perform oversampling using RandomOverSampler\n",
    "# oversampler = RandomOverSampler()\n",
    "# X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature scaling using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "input_model = sc_X.fit_transform(input_coba)\n",
    "\n",
    "\n",
    "# Train the SVM (Support Vector Machine) classifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# load model\n",
    "# classifier = SVC(kernel='poly',gamma=\"scale\", C=2.0)\n",
    "# classifier.fit(X_train_resampled, y_train_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
