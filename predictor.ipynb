{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy.stats\n",
    "from sklearn import metrics, preprocessing\n",
    "import pandas as pd\n",
    "from keras.utils import audio_dataset_from_directory\n",
    "from scipy.io import wavfile\n",
    "from IPython import display\n",
    "import joblib\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# # Define the path to the dataset directory\n",
    "# DATASET_PATH = \"C:/Users/ITPKL/Desktop/pydev/ccw_sound\"\n",
    "# data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "# # Create training and validation dataset\n",
    "# train_ds, val_ds =  audio_dataset_from_directory(\n",
    "#     directory=data_dir,\n",
    "#     batch_size=64,\n",
    "#     validation_split=0.3,\n",
    "#     seed=0,\n",
    "#     output_sequence_length=16000,\n",
    "#     subset='both'\n",
    "# )\n",
    "\n",
    "# # Retrieve the class names from the training dataset\n",
    "# label_names = np.array(train_ds.class_names)\n",
    "# label_names_val = np.array(val_ds.class_names)\n",
    "\n",
    "\n",
    "# # Print the label names\n",
    "# print()\n",
    "# print(\"label names training:\", label_names)\n",
    "# print(\"label names testing:\", label_names_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input file pipeline (test)/ membaca satu file audio dari fungsi utama dan mengembalikan dalam bentuk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove singleton dimensions from audio tensor\n",
    "# def squeeze(audio, labels):\n",
    "#     audio = tf.squeeze(audio, axis=-1)\n",
    "#     return audio, labels\n",
    "\n",
    "# # Apply the squeeze function to the training and validation datasets\n",
    "# train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "# val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "# Define a function to read WAV files and convert to FFT\n",
    "def read_wav_file(file_path, fft_length=1024):\n",
    "    try:\n",
    "        sample_rate, audio = wavfile.read(file_path)\n",
    "        if len(audio) == 0:\n",
    "            return None\n",
    "        return np.fft.fft(audio)[:fft_length]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading WAV file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert fft data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dframe(fmd, pm):\n",
    "    # Create a DataFrame with the magnitude of the FFT outputs and numerical labels\n",
    "    fft_magnitude_df = pd.DataFrame(fmd)\n",
    "    mask_df = pd.DataFrame(pm)\n",
    "    # Rename the columns of the DataFrame to fft1, fft2, ...\n",
    "    # new_columns = [f'fft{n}' for n in range(1, fft_length + 1)]\n",
    "    # fft_magnitude_df.columns = new_columns\n",
    "\n",
    "    # Add a new column 'Label' to the DataFrame based on the folder name\n",
    "    # fft_magnitude_df['label'] = labels\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # display.display(fft_magnitude_df)\n",
    "\n",
    "    # fft_magnitude_tuples = [tuple(row) for row in fft_magnitude_df.values]\n",
    "    # random.shuffle(fft_magnitude_tuples)\n",
    "    # Display the shuffled list of tuples\n",
    "        # for tup in fft_magnitude_tuples:\n",
    "        # print(tup)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return fft_magnitude_df, mask_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proses 1 file audio yang sudah di preprocess menjadi fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process a single WAV file\n",
    "def proses_file_audio(file_path, fft_length=1024):\n",
    "    fft_magnitude_data = []\n",
    "    masks = []\n",
    "    \n",
    "    # masuk fungsi baca file untuk mengembalikan dalam bentuk data fft \n",
    "    fft = read_wav_file(file_path, fft_length)\n",
    "    if fft is not None:\n",
    "        fft_magnitude = np.abs(fft)\n",
    "        mask = np.ones_like(fft_magnitude)\n",
    "        mask[len(fft_magnitude):] = 0\n",
    "        fft_magnitude_padded = np.pad(fft_magnitude, (0, fft_length - len(fft_magnitude)))\n",
    "        mask_padded = np.pad(mask, (0, fft_length - len(mask)))\n",
    "        \n",
    "        # Append the padded FFT magnitude and mask to their respective lists\n",
    "        fft_magnitude_data.append(fft_magnitude_padded)\n",
    "        masks.append(mask_padded)\n",
    "\n",
    "    # Initialize an empty list to store the padded masks\n",
    "    padded_masks = [np.pad(mask, (0, fft_length - len(mask))) for mask in masks]\n",
    "\n",
    "    # Find the maximum length among all padded masks\n",
    "    max_length = max(len(mask) for mask in padded_masks)\n",
    "\n",
    "    # Pad each mask to the maximum length\n",
    "    padded_masks = [np.pad(mask, (0, max_length - len(mask))) for mask in padded_masks]\n",
    "\n",
    "    # Convert the lists to numpy arrays\n",
    "    fft_magnitude_data = np.array(fft_magnitude_data)\n",
    "    padded_masks = np.array(padded_masks)\n",
    "\n",
    "    # Print the shapes of the resulting arrays\n",
    "    print(\"FFT Magnitude Data Shape:\", fft_magnitude_data.shape)\n",
    "    print(\"Masks Shape:\", padded_masks.shape)\n",
    "\n",
    "    \n",
    "    hasil_magnitude , hasil_padded = dframe(fft_magnitude_data, padded_masks)\n",
    "    \n",
    "\n",
    "\n",
    "    # belum terbentuk numpy array dan masih dalam bentuk dataframe\n",
    "    print(hasil_magnitude)\n",
    "    print()\n",
    "    print(hasil_padded) #padded mask dari fungsi\n",
    "    print('===================================================================')\n",
    "\n",
    "    #sudah terbentuk numpy array  \n",
    "    print(fft_magnitude_data)\n",
    "    print()\n",
    "    print(padded_masks)\n",
    "    print()\n",
    "\n",
    "    # import file csv\n",
    "    X_coba = pd.read_csv('X_coba.csv')\n",
    "    y_coba = pd.read_csv('y_coba.csv') # ubah disini\n",
    "    X_train_resampled = pd.read_csv('X_train_resampled.csv')\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc_X = StandardScaler()\n",
    "\n",
    "    X_train_resampled = sc_X.fit_transform(X_train_resampled)\n",
    "    X_coba = sc_X.transform(X_coba)\n",
    "    scaled_input = sc_X.transform(hasil_magnitude)\n",
    "\n",
    "    # Load the model\n",
    "    model = joblib.load('C:/Users/ITPKL/Desktop/pydev/model20240130.pkl')\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(scaled_input) # ganti disini untuk melihat test\n",
    "    # print(y_pred) \n",
    "\n",
    "    \n",
    "    if y_pred == 0:\n",
    "        print(\"Prediksi model: NG\")\n",
    "    else:\n",
    "        print(\"Prediksi model: OK\")\n",
    "\n",
    "    # from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # accuracy = accuracy_score(y_coba, y_pred)\n",
    "\n",
    "    # print(\"Accuracy train:\", accuracy)\n",
    "\n",
    "    # from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    # print(classification_report(y_coba,y_pred,zero_division=1))\n",
    "    # cm = confusion_matrix(y_coba, y_pred)\n",
    "\n",
    "    # sns.heatmap(cm, annot=True, yticklabels=True,cbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFT Magnitude Data Shape: (1, 1024)\n",
      "Masks Shape: (1, 1024)\n",
      "     0             1             2            3            4          5     \\\n",
      "0  8032.0  11460.938428  11421.040877  7779.445288  7531.039369  6576.5519   \n",
      "\n",
      "          6            7            8            9     ...        1014  \\\n",
      "0  4727.083644  3630.904414  1505.560959  2601.330374  ...  281.149782   \n",
      "\n",
      "         1015        1016        1017       1018        1019        1020  \\\n",
      "0  250.759502  173.559206  206.235498  293.06516  360.332295  736.719573   \n",
      "\n",
      "         1021      1022        1023  \n",
      "0  220.779597  373.8634  255.100736  \n",
      "\n",
      "[1 rows x 1024 columns]\n",
      "\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  1014  \\\n",
      "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   1.0   \n",
      "\n",
      "   1015  1016  1017  1018  1019  1020  1021  1022  1023  \n",
      "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  \n",
      "\n",
      "[1 rows x 1024 columns]\n",
      "===================================================================\n",
      "[[ 8032.         11460.9384282  11421.04087667 ...   220.7795968\n",
      "    373.86340037   255.10073646]]\n",
      "\n",
      "[[1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      "Prediksi model: OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ITPKL\\Desktop\\pydev\\venv\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# manggil input\n",
    "proses_file_audio('testing_model/test_ok/CCW_346.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read audio, transform to FFT, and process with the ML model\n",
    "# def analyze_audio():\n",
    "    \n",
    "    # audio_data, sample_rate = sf.read('recorded_audio.wav')\n",
    "\n",
    "    # # Apply noise reduction\n",
    "    # # reduced_noise = nr.reduce_noise(audio_clip=audio_data, noise_clip=noise_data)\n",
    "    \n",
    "    # if sample_rate != 44100:\n",
    "    #     audio_data = sf.resample(audio_data, 44100)\n",
    "    \n",
    "    # # Mengubah audio menjadi mono jika perlu\n",
    "    # if len(audio_data.shape) > 1:\n",
    "    #     audio_data = np.mean(audio_data, axis=1)\n",
    "    \n",
    "    # # Normalisasi data audio\n",
    "    # audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "    # # ======================================================\n",
    "    # # audio_data = np.frombuffer(stream.read(RATE*5), dtype=np.int16)  # Record for 1 second\n",
    "    # # fft_data = np.abs(np.fft.fft(audio_data))[:RATE // 2]\n",
    "    # # ===========================================\n",
    "    # # Memisahkan audio menjadi chunk-chunk kecil\n",
    "    # chunk_size = 2048  # Increase chunk_size to match the expected number of features\n",
    "    # num_chunks = len(audio_data) // chunk_size\n",
    "    # chunks = np.array_split(audio_data[:num_chunks * chunk_size], num_chunks)\n",
    "    \n",
    "    # # Memproses setiap chunk dan membuat prediksi\n",
    "    # for chunk in chunks:\n",
    "    #     fft_data = np.abs(np.fft.fft(chunk))[:chunk_size // 2]\n",
    "        \n",
    "    #     # Mengubah format data FFT sesuai dengan model yang dilatih\n",
    "    #     input_data = fft_data.reshape(1, -1)  # Karena model membutuhkan input 2D\n",
    "        \n",
    "    #     # Menggunakan model untuk membuat prediksi\n",
    "    #     prediction = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def dframe ():\n",
    "#     # Create a DataFrame with the magnitude of the FFT outputs and numerical labels\n",
    "#     fft_magnitude_df = pd.DataFrame(fft_magnitude_data)\n",
    "\n",
    "#     # Rename the columns of the DataFrame to fft1, fft2, ...\n",
    "#     # new_columns = [f'fft{n}' for n in range(1, fft_length + 1)]\n",
    "#     # fft_magnitude_df.columns = new_columns\n",
    "\n",
    "#     # Add a new column 'Label' to the DataFrame based on the folder name\n",
    "#     # fft_magnitude_df['label'] = labels\n",
    "\n",
    "#     # Display the DataFrame\n",
    "#     # display.display(fft_magnitude_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes all fft values as X and drops the Label\n",
    "# X = fft_magnitude_df.drop(['label'], axis=1)\n",
    "# # y = fft_magnitude_df['label']\n",
    "\n",
    "# input_1_image = np.array(X)\n",
    "# label_1_image = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splits the datasset into 80% training set and 20% test set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# Splits the dataset into 80% training set and 20% test set\n",
    "# input_latih, input_coba, label_latih, label_coba = train_test_split(input_1_image, label_1_image, test_size=0.2, random_state=0)\n",
    "\n",
    "# Perform oversampling using RandomOverSampler\n",
    "# oversampler = RandomOverSampler()\n",
    "# X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature scaling using StandardScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc_X = StandardScaler()\n",
    "# input_model = sc_X.fit_transform(input_coba)\n",
    "\n",
    "\n",
    "# Train the SVM (Support Vector Machine) classifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# load model\n",
    "# classifier = SVC(kernel='poly',gamma=\"scale\", C=2.0)\n",
    "# classifier.fit(X_train_resampled, y_train_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
